您好！恭喜您选择了一篇非常前沿且重要的多模态视觉论文。作为新手，您能直接阅读这类文献非常棒！

这篇论文《Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs (PaDT：面向多模态大语言模型统一多模态视觉任务的“可解码块级标记”)》提出了一种创新的方法，旨在解决现有大模型在执行视觉感知任务时效率低、难以统一的问题。

我为您精读并详细解释论文的每一部分内容，帮助您理解它的核心思想、方法和贡献。

论文精读与详细解释

标题与基本信息

 * 标题： Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs

   * 核心思想： 将图像的“块”（Patch，即图像的基本视觉单元）转化为 LLM 可以直接“解码”的“标记”（Token）。

   * 目标： 在多模态大语言模型（MLLMs）中实现统一的多模态视觉任务处理。

 * 基本信息： 论文于 2025 年 10 月 2 日发布在 arXiv 上，目前正在审稿中。

摘要 (ABSTRACT)

这段介绍了问题、核心方法、关键组件、优势和主要成果。

 * 背景与问题 (Problem Statement)：

   * 多模态大语言模型（MLLMs）发展迅速。

   * 现有问题： 现有的 MLLMs 在处理视觉任务时，通常依赖间接表示，例如将目标的边界框坐标生成为文本（如 [x1, y1, x2, y2]）。

   * 后果： 这种间接方式限制了模型的性能，并且无法处理密集预测任务，比如图像分割（Segmentation）。

 * 核心方法 (The PaDT Paradigm)：

   * 为了克服这些挑战，论文提出了 Patch-as-Decodable Token (PaDT)。

   * PaDT 是一个统一的范式： 它使 MLLMs 能够直接生成文本输出和多样化的视觉输出。

 * 关键创新点 (Key Components)：

   * Visual Reference Tokens (VRTs) / 视觉参考标记： 这是 PaDT 的核心。VRTs 是从查询图像的视觉块嵌入（Visual Patch Embeddings，即图像块的特征表示）中提取出来的。

   * 工作方式： VRTs 被无缝地穿插（interleaved）在 LLM 的输出文本标记中。

   * 轻量级解码器： 一个附加的轻量级解码器负责将 MLLM 输出的 VRTs 转换为最终的结构化视觉预测，包括检测（Detection）、分割（Segmentation）和接地（Grounding）。

 * 方法优势 (Advantages)：

   * 与现有方法不同，PaDT 在每次前向传播中独立处理 VRTs。

   * 它动态扩展了模型的嵌入表（Embedding Table）。

   * 这大大提高了模型的定位能力和区分相似物体的能力。

 * 训练策略与成果 (Training and Results)：

   * 定制了训练策略，随机选择 VRTs 进行微调，并引入了鲁棒的逐标记交叉熵损失（per-token cross-entropy loss）。

   * 成果： 在四个视觉感知和理解任务上的研究表明，PaDT 持续获得了最先进的性能，甚至超越了参数规模大得多的 MLLM 模型。

第 1 节：引言 (1 INTRODUCTION)

这段详细阐述了现有方法的局限性，并引出了 PaDT 及其关键贡献。

 * 视觉任务的重要性与传统模型 (Background)：

   * 目的： 细粒度的图像感知旨在将特定图像区域与语义或实例信息关联起来，是计算机视觉的基石。

   * 传统模型局限： 尽管传统视觉模型在纯检测和分割上表现优异，但它们缺乏灵活的语言交互和理解能力，无法进行开放词汇（Open Vocabulary）的视觉推理。

 * 现有 MLLMs 的挑战 (Challenges of Existing MLLMs)：

   * 当前 MLLMs（如 Qwen2.5-VL）为了适应 LLM 的文本输出空间，将检测区域序列化为文本形式的边界框坐标。

   * 挑战一：输出格式不一致 (Inconsistent Output)： 即使在相同的提示下，坐标的表示方式也可能不同（例如，浮点数 vs 整数，JSON 格式 vs 自由格式），这使得解析和结构化输出变得困难（如图 2(a)）。

   * 挑战二：语义和空间不连续 (Semantic & Spatial Discontinuity)：

     * 数值坐标缺乏文本和视觉模态之间的语义对齐（如图 2(b)）。这种不匹配可能导致重复或幻觉。

     * 一个连续的坐标值（如 489）被分解成离散的文本标记（如 4, 8, 9），破坏了数值的连续性，影响预测精度。

 * PaDT 统一范式 (PaDT's Unified Paradigm)：

   * 核心： PaDT 旨在使 MLLMs 能够以统一且灵活的方式，直接生成文本和多样化的视觉目标。

   * VRTs 的角色： VRTs 由动态嵌入模块从原始视觉块嵌入中生成，因此它们具有与 LLM 一致的特征空间，并且每个 VRT 都明确对应于查询图像中的特定图像块。

 * 与先前工作的对比 (Contrast with Prior Work)：

   * 有先前工作（如 Claw Machine）尝试让 LLM 输出图像块标记，但它们依赖于全局码本（Global Codebook）。

   * 全局码本的局限： 缺乏灵活性和通用性，可能预测出图像中不存在的标记，或者由于视觉相似性导致对象间的混淆。

   * PaDT 的优势： PaDT 在每次前向传播中独立处理 VRTs，并保持每个图像区域独特的定位信息，确保了预测的连贯性（如图 2(c) 和 2(d) 所示 VRT 的空间连续性）。

 * 总结贡献 (Main Contributions)：

   * 范式： 提出了 PaDT，通过 VRTs 实现文本和多样化视觉目标的统一生成。

   * 解码器： 提出了轻量级且鲁棒的 PaDT 解码器，可统一解码分割掩码和边界框等结构化视觉输出。

   * 训练： 提出了有效的微调策略和鲁棒的逐标记交叉熵损失。

   * 结果： 在广泛的视觉感知和理解任务上实现了最先进的性能。

第 2 节：相关工作 (2 RELATED WORK)

这段介绍了 MLLMs 的发展历程，以及解决细粒度视觉任务的现有方法。

 * 多模态大语言模型 (MLLMs)：

   * 从 CLIP 和 ALIGN 等的对比预训练奠定基础。

   * 到 BLIP-2 通过 Q-Former 改进对齐。

   * 再到 LLaVA 和 MiniGPT-4 等的指令微调模型，实现开放式的视觉问答和推理。

   * 最新工作扩展到高分辨率、多图像和视频输入。

 * MLLMs 用于视觉感知与理解 (Visual Perception & Understanding)：

   * 问题： 通用 MLLMs 在细粒度感知任务上仍有局限，因为视觉编码器依赖固定的块网格，容易模糊局部细节。

   * 解决方案：

     * 自适应策略： 采用自适应平铺策略（如 NaViT-style）来处理可变分辨率图像块。

     * 强化学习： 结合强化学习（如 VLM-R1）来增强感知和推理能力。

   * 开放挑战： 设计一个统一的架构来无缝地适应多样化的视觉任务仍然是一个挑战。

 * 统一视觉标记化 (Unified Visual Tokenization)：

   * 区域级别： 将目标框或掩码转换为几何标记或可学习的代理（proxies）。

   * 块级别： 使用 CLIP 衍生的块特征作为视觉词汇。

   * PaDT 的定位： 现有方法在视觉和语言的深层语义整合方面仍受限。PaDT 提出了动态多模态标记空间，在统一的自回归建模下，实现语言标记和视觉块之间的紧密对应。

第 3 节：方法论 (3 METHODOLOGY)

这段是论文的技术核心，详细解释了 PaDT 框架的三个关键组成部分。

3.1 回顾 MLLM 的局限性

 * MLLM 流程： 视觉编码器 f_v 将图像转换为块嵌入 F_v，投影器 f_p 转换为 F_{patch}。F_{patch} 与文本嵌入 E_{text}(T) 融合为混合表示 Z，输入 LLM。LLM 通过分类器权重 W_{text} 生成文本输出。

 * 局限重申： MLLMs 仅限于生成文本输出，这不利于对象检测和分割等结构化视觉任务。基于数值坐标的文本序列化存在格式不一致、数值不连续和缺乏语义对齐等重大问题。

3.2 视觉参考标记 (VISUAL REFERENCE TOKEN)

PaDT 框架通过三个组件解决了上述问题：

3.2.1 统一多模态格式与 VRTs

 * 动态多模态码本扩展 (Dynamic Multi-Modal Codebook Expansion)：

   * 目的： 确保 VRTs 既能被 LLM 理解（可嵌入），又能作为输出（可解码）。

   * 避免全局码本： PaDT 不使用固定的全局码本，而是重用当前输入图像中提取的视觉标记。

   * 实现：

     * 原始块特征 F_{patch} 通过一个轻量级模块 f_{vp}（由 LayerNorm 和低秩线性投影组成）投影成视觉参考原型 \mathcal{P}_{ref}。

     * \mathcal{P}_{ref} 随后与原始文本嵌入 E_{text} 拼接，形成了动态嵌入表 E_{dyn}=[E_{text};\mathcal{P}_{ref}]。这意味着每次输入图像不同时，码本都会动态更新。

 * 统一的输入和输出格式 (Unified I/O Format)：

   * 输入侧： 图像标记被索引到 E_{dyn} 中，作为 VRTs 嵌入到 LLM 的输入序列中。

   * 输出侧 (PaDT Head)： 提出了 PaDT 头，用 \mathcal{P}_{ref} 增强原有的文本分类器 W_{text}，得到 W_{tv}=[W_{text};\mathcal{P}_{ref}]。这使得 LLM 能够在其输出空间中直接预测 VRTs 的索引，从而在自回归序列中插入块级别的视觉参考。

   * 表示策略： 检测到的对象由其上的部分（而非全部）VRTs 来表示。

3.2.2 轻量级 PaDT 解码器 (LIGHT-WEIGHT PADT DECODER)

 * 目的： 将 MLLM 输出的（部分）VRTs 转换成任务所需的边界框或掩码等结构化输出。

 * 结构： 采用了一个由三个双向注意力块（Two-way Attention Blocks）组成的堆栈。

 * 输入： 接收来自 LLM 最后一层的预测 VRTs 的隐藏特征，这些特征被组织成对象查询（Object Queries），即对应于一个检测目标的 VRT 序列。

 * 解码过程： 将三个可学习的任务标记（边界框标记、掩码标记和分数标记）注入到每组对象查询中。经过注意力块后，每个任务标记被投影到其对应的输出空间，产生边界框、分割掩码和置信度分数。

3.2.3 训练策略 (TRAINING STRATEGY)

 * 鲁棒的逐标记交叉熵损失 (\mathcal{L}_{CE}^{robust})：

   * 关键创新： 采用监督微调，但随机采样 N_{vrt} 个前景视觉标记作为真值监督，而不是使用所有前景标记。

   * 目的： 增加监督的多样性，防止模型对固定标记集过拟合。

   * 实现： 对于未被选择的真值标记，在计算损失时会通过掩码（将 Logits 设为 -\infty）来抑制它们对 Softmax 的贡献，使其既不被奖励也不被惩罚。

 * 任务特定损失：

   * 对于 PaDT 解码器的结构化输出，采用了任务特定目标函数：\mathcal{L}_{bbox}（边界框损失）、\mathcal{L}_{mask}（掩码损失）和 \mathcal{L}_{score}（分数损失）。

   * 最终总损失： L = \mathcal{L}_{CE}^{robust} + \mathcal{L}_{bbox} + \mathcal{L}_{mask} + \mathcal{L}_{score}。

第 4 节：实验 (4 EXPERIMENT)

这段展示了 PaDT 在多个视觉任务上的卓越性能。

 * 任务和数据集 (Tasks and Datasets)：

   * 指称表达理解 (REC) 和分割 (RES)： 在 RefCOCO, RefCOCO+, RefCOCOg 上进行。

   * 开放词汇检测 (OVD)： 在 COCO 2017 上进行。

   * 指称图像描述 (RIC)： 一个新构建的基准。

 * 架构与训练 (Details)：

   * 基础模型： 采用了 Qwen2.5-VL 的 3B 和 7B 版本。

   * VRT 采样： 每一步随机采样 N_{vrt}=5 个前景 VRTs。

 * 多任务可扩展性 (Multi-Task Scalability)：

   * PaDT Pro： PaDT 的增强多任务变体，通过在所有基准上联合训练得到。

   * 优势： 联合训练持续提高了性能，且 PaDT Pro 只需改变提示即可无缝切换任务。

 * 实验结果 (Key Results)：

   * REC (指称表达理解)： PaDT Pro (3B) 的总体准确率达到 93.6，优于参数规模大得多的 78B InternVL3 模型。

   * RES (指称表达分割)： PaDT Pro 在 cIoU 指标上表现最佳，超越了使用强大分割模型 SAM 的方法。

   * OVD (开放词汇检测)： PaDT Pro 相比基线模型 Qwen2.5-VL 有显著提升。

   * 总结： 实验结果充分证明了 VRT 范式的有效性，它能将文本语义与图像块对齐，从而显著提高了 MLLMs 的对象定位精度。

![](https://com.miui.notes/note_image/634b2161d2a608ad55c57f24b84555733942bbd2)



