# -*- coding: utf-8 -*-
"""
å®Œæ•´æœ¬åœ°ç‰ˆæ¨ç†è„šæœ¬ï¼ˆYOLO + VLM-FO1ï¼Œè‹±æ–‡æ—¥å¿—ç‰ˆï¼‰
- YOLO æœ‰æ¡† â†’ ä½¿ç”¨çœŸå®ç±»åˆ«ï¼ˆfire=0, smoke=1ï¼‰ï¼Œå°†æ¡†åŒºåŸŸé€å…¥ VLM è®¡æ•°
  - è‹¥ VLM è®¡æ•° > 1ï¼Œåˆ™ç½®ä¿¡åº¦æå‡è‡³ 0.9
- YOLO æ— æ¡† â†’ 3x3 ç½‘æ ¼ â†’ VLM æ£€æµ‹ fire/smoke
- è‹¥æ£€æµ‹åˆ°ç›®æ ‡ â†’ ç»˜åˆ¶å¯¹åº”é¢œè‰²æ¡†ï¼ˆç«ç„°: ç»¿, çƒŸé›¾: çº¢ï¼‰
  - YOLO æ¨¡å¼ï¼šä½¿ç”¨è°ƒæ•´åçš„ç½®ä¿¡åº¦
  - ç½‘æ ¼æ¨¡å¼ï¼šVLM ç»“æœï¼Œç½®ä¿¡åº¦å›ºå®šä¸º 0.5
- å§‹ç»ˆç”Ÿæˆã€Œfireã€ã€Œsmokeã€çš„è‹±æ–‡è¯¦ç»†æè¿°ï¼ˆåŸºäºæ•´å›¾ï¼‰ï¼Œå¹¶ç›´æ¥åŒ…å«åœ¨æ—¥å¿—ä¸­
- ç»ˆç«¯æ—¥å¿—ä¸ºè‹±æ–‡
- æ¯å¼ å›¾çš„å¯è§†åŒ–ç»“æœä¿å­˜ï¼Œæ‰€æœ‰æ—¥å¿—æ±‡æ€»åˆ° total_processing_log.txt
"""

import os
import re
import sys
import signal

# ================== YOLOv5 è·¯å¾„ ==================
YOLOV5_LOCAL_DIR = "/root/autodl-tmp/yolov5/yolov5-master"
if not os.path.exists(YOLOV5_LOCAL_DIR):
    raise FileNotFoundError(f"âŒ YOLOv5 è·¯å¾„ä¸å­˜åœ¨: {YOLOV5_LOCAL_DIR}")

models_path = os.path.join(YOLOV5_LOCAL_DIR, "models")
if not os.path.exists(models_path):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° YOLOv5 çš„ models æ–‡ä»¶å¤¹ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {models_path}")

sys.path.insert(0, YOLOV5_LOCAL_DIR)
print(f"âœ… å·²æˆåŠŸæ·»åŠ  YOLOv5 è·¯å¾„: {YOLOV5_LOCAL_DIR}")

import torch
import cv2
import numpy as np
from PIL import Image

# æœ‰äº›ä»“åº“ç»“æ„æ˜¯ /yolov5/yolov5-masterï¼Œè¿™é‡Œä¹ŸæŠŠä¸Šä¸€çº§åŠ ä¸€ä¸‹
YOLOV5_PARENT_DIR = "/root/autodl-tmp/yolov5"
sys.path.insert(0, YOLOV5_PARENT_DIR)

# ================== å¯¼å…¥ YOLOv5 ç›¸å…³æ¨¡å— ==================
from models.experimental import attempt_load
from utils.general import non_max_suppression, scale_boxes
from utils.augmentations import letterbox

# ================== å¯¼å…¥ VLM-FO1 ç›¸å…³æ¨¡å— ==================
from vlm_fo1.model.builder import load_pretrained_model
from vlm_fo1.mm_utils import prepare_inputs, extract_predictions_to_bboxes

# ================== åŸºæœ¬é…ç½® ==================
image_folder = "/root/autodl-tmp/datasets_input/images"  # è¾“å…¥å›¾åƒç›®å½•
output_folder = "/root/autodl-tmp/datasets_output"       # è¾“å‡ºç»“æœç›®å½•
model_path = "/root/autodl-tmp/resources/resources/VLM-FO1_Qwen2.5-VL-3B-v01"
wheel_path = "/root/autodl-tmp/resources/resources/flash_attn-2.8.0+cu124torch2.6-cp311-cp311-linux_x86_64.whl"
yolov5_weights_path = "/root/autodl-tmp/exp8best/weights/best.pt"

os.makedirs(output_folder, exist_ok=True)

# å¦‚æœå­˜åœ¨ flash_attn è½®å­ï¼Œå°±é™é»˜å®‰è£…ä¸€ä¸‹ï¼ˆå¯é€‰ï¼‰
if os.path.exists(wheel_path):
    print(f"ğŸ”§ æ£€æµ‹åˆ° flash_attn whl åŒ…ï¼Œæ­£åœ¨å®‰è£…ï¼š{wheel_path}")
    os.system(f"pip install -q {wheel_path}")

# ================== åŠ è½½æ¨¡å‹ ==================
print("ğŸ”¥ æ­£åœ¨åŠ è½½ VLM-FO1 æ¨¡å‹...")
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer, model, image_processors = load_pretrained_model(
    model_path,
    load_8bit=False,
    load_4bit=False,
    device=device,
)

print("ğŸ”¥ æ­£åœ¨åŠ è½½ YOLOv5 æ¨¡å‹æƒé‡...")
yolov5_model = attempt_load(yolov5_weights_path, device=device)
yolov5_model.eval()

# ç±»åˆ« IDï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
fire_label = 0   # å¯¹åº”ç«ç„°
smoke_label = 1  # å¯¹åº”çƒŸé›¾

# ================== å·¥å…·å‡½æ•° ==================
def parse_count_from_text(text: str) -> int:
    """ä» VLM çš„æ–‡æœ¬è¾“å‡ºä¸­è§£ææ•°é‡ï¼Œå°½é‡é²æ£’ä¸€ç‚¹"""
    text_lower = text.lower().strip()

    # æ˜ç¡®çš„å¦å®šæƒ…å†µ
    if any(w in text_lower for w in ["no ", "no.", "none", "not ", "zero", "not detected", "no fire", "no smoke"]):
        return 0

    # è‹±æ–‡æ•°é‡è¯æ˜ å°„
    word_to_num = {
        "one": 1,
        "two": 2,
        "three": 3,
        "four": 4,
        "five": 5,
        "six": 6,
        "seven": 7,
        "eight": 8,
        "nine": 9,
        "ten": 10,
        "multiple": 2,
        "several": 3,
        "many": 5,
        "a lot": 5,
    }
    for word, num in word_to_num.items():
        if word in text_lower:
            return num

    # æå–é˜¿æ‹‰ä¼¯æ•°å­—
    numbers = re.findall(r"\d+", text)
    if numbers:
        try:
            return int(numbers[0])
        except ValueError:
            pass

    # å¦‚æœæåˆ°äº† fire/smoke/flame ç­‰å…³é”®è¯ä½†æ²¡ç»™æ•°é‡ï¼Œé»˜è®¤ 1
    if any(obj in text_lower for obj in ["fire", "smoke", "flame", "burn","burning"]):
        return 1

    # å®åœ¨è§£æä¸å‡ºæ¥å°±è®¤ä¸º 0
    return 0

def get_yolov5_bboxes(model, img_path, conf_thresh=0.3):
    """å¯¹å•å¼ å›¾ç‰‡ä½¿ç”¨ YOLOv5 æ¨ç†ï¼Œè¿”å› [x1, y1, x2, y2, conf, cls_id] åˆ—è¡¨å’ŒåŸå›¾ shape"""
    img0 = cv2.imread(img_path)
    if img0 is None:
        return [], None

    # letterbox é¢„å¤„ç†
    img = letterbox(img0, 640, stride=32, auto=True)[0]
    img = img.transpose((2, 0, 1))
    img = np.ascontiguousarray(img)

    img = torch.from_numpy(img).to(device).float() / 255.0
    if img.ndimension() == 3:
        img = img.unsqueeze(0)

    with torch.no_grad():
        pred = model(img)[0]

    # NMS
    pred = non_max_suppression(pred, conf_thresh, 0.45, classes=None, agnostic=False)

    bboxes = []
    for det in pred:
        if len(det):
            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()
            for *xyxy, conf, cls in det:
                if float(conf) > conf_thresh:
                    x1, y1, x2, y2 = map(int, xyxy)
                    cls_id = int(cls)
                    bboxes.append([x1, y1, x2, y2, float(conf), cls_id])

    return bboxes, img0.shape

def detect_and_count(object_name_en: str, bbox_list, img_path):
    """
    ä½¿ç”¨ VLM-FO1 è¿›è¡Œç›®æ ‡æ£€æµ‹ + è®¡æ•°
    object_name_en: "fire" æˆ– "smoke"ï¼ˆè‹±æ–‡ï¼Œç”¨äºæ¨¡æ¿å’Œè§£æï¼‰
    bbox_list: [[x, y, w, h], ...]ï¼Œç»™ VLM çš„å€™é€‰åŒºåŸŸ
    """
    # æ£€æµ‹æ¶ˆæ¯ï¼ˆå¸¦ bboxï¼‰
    # âœ… æå‰æ ¼å¼åŒ– f-string
    detect_prompt = f"è¯·åœ¨å›¾åƒä¸­æ£€æµ‹æ‰€æœ‰çš„ {object_name_en}ã€‚è¯·åˆ—å‡ºå®ƒä»¬åœ¨ä»¥ä¸‹åŒºåŸŸä¸­çš„ä½ç½®ï¼ˆx,y,w,hï¼‰ï¼š{bbox_list}ã€‚å¦‚æœæ²¡æœ‰ï¼Œè¯·è¿”å›ç©ºåˆ—è¡¨ []ã€‚"
    detect_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": detect_prompt},
            ],
            "bbox_list": bbox_list,
        }
    ]

    # è®¡æ•°æ¶ˆæ¯ï¼ˆåŒæ ·ä½¿ç”¨è‹±æ–‡ï¼Œä¿æŒå’Œ parse_count ä¸€è‡´ï¼‰
    count_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {
                    "type": "text",
                    "text": f"å›¾åƒä¸­æ€»å…±æœ‰å¤šå°‘ä¸ª {object_name_en}ï¼Ÿè¯·ç›´æ¥å›ç­”ä¸€ä¸ªæ•°å­—ã€‚",
                },
            ],
            "bbox_list": bbox_list, # ä¿æŒä¸€è‡´æ€§ï¼Œè™½ç„¶è®¡æ•°å¯èƒ½ä¸ä¾èµ–bbox
        }
    ]

    # æ£€æµ‹æ¨ç†
    detect_kwargs = prepare_inputs(
        model_path,
        model,
        image_processors,
        tokenizer,
        detect_messages,
        max_tokens=4096,
        top_p=0.05,
        temperature=0.0,
        do_sample=False,
    )
    # --- è¶…æ—¶å¤„ç† ---
    def timeout_handler(signum, frame):
        raise TimeoutError("VLM detection æ¨ç†è¶…æ—¶")

    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(30)  # è®¾ç½® 30 ç§’è¶…æ—¶
    try:
        with torch.inference_mode():
            detect_output_ids = model.generate(**detect_kwargs)
            detect_outputs = tokenizer.decode(
                detect_output_ids[0, detect_kwargs["inputs"].shape[1]:]
            ).strip()
    except TimeoutError:
        print("âŒ VLM detection æ¨ç†è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æµ‹")
        detect_outputs = "[]"
    finally:
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        signal.signal(signal.SIGALRM, old_handler) # æ¢å¤ä¿¡å·å¤„ç†
    # --- è¶…æ—¶å¤„ç†ç»“æŸ ---

    label_to_bboxes = extract_predictions_to_bboxes(detect_outputs, bbox_list)
    # VLM è¿”å›çš„ label ä¸€èˆ¬ç”¨å°å†™è‹±æ–‡
    bboxes = label_to_bboxes.get(object_name_en.lower(), [])

    # è®¡æ•°æ¨ç†
    count_kwargs = prepare_inputs(
        model_path,
        model,
        image_processors,
        tokenizer,
        count_messages,
        max_tokens=512, # è®¡æ•°ä¸éœ€è¦å¤ªé•¿
        top_p=0.95,
        temperature=0.7,
        do_sample=True,
    )
    # --- è¶…æ—¶å¤„ç† ---
    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(15)  # è®¾ç½® 15 ç§’è¶…æ—¶
    try:
        with torch.inference_mode():
            count_output_ids = model.generate(**count_kwargs)
            count_outputs = tokenizer.decode(
                count_output_ids[0, count_kwargs["inputs"].shape[1]:]
            ).strip()
    except TimeoutError:
        print("âŒ VLM count æ¨ç†è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡è®¡æ•°")
        count_outputs = "0"
    finally:
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        signal.signal(signal.SIGALRM, old_handler) # æ¢å¤ä¿¡å·å¤„ç†
    # --- è¶…æ—¶å¤„ç†ç»“æŸ ---

    count = parse_count_from_text(count_outputs)
    return bboxes, count

def detect_count_in_single_bbox(object_name_en: str, x1, y1, x2, y2, img_path):
    """
    ä½¿ç”¨ VLM-FO1 å¯¹å•ä¸ª YOLO æ¡†åŒºåŸŸè¿›è¡Œè®¡æ•°
    object_name_en: "fire" æˆ– "smoke"
    x1, y1, x2, y2: YOLO æ¡†çš„åæ ‡
    """
    # è®¡æ•°æ¶ˆæ¯ï¼ˆåªé’ˆå¯¹å•ä¸ª bboxï¼‰
    count_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {
                    "type": "text",
                    "text": f"åœ¨åŒºåŸŸ [{x1}, {y1}, {x2 - x1}, {y2 - y1}] å†…ï¼Œæ€»å…±æœ‰å¤šå°‘ä¸ª {object_name_en}ï¼Ÿè¯·ç›´æ¥å›ç­”ä¸€ä¸ªæ•°å­—ã€‚",
                },
            ],
            "bbox_list": [[x1, y1, x2 - x1, y2 - y1]], # åªåŒ…å«è¿™ä¸€ä¸ªæ¡†
        }
    ]

    # è®¡æ•°æ¨ç†
    count_kwargs = prepare_inputs(
        model_path,
        model,
        image_processors,
        tokenizer,
        count_messages,
        max_tokens=512,
        top_p=0.95,
        temperature=0.7,
        do_sample=True,
    )
    # --- è¶…æ—¶å¤„ç† ---
    def timeout_handler(signum, frame):
        raise TimeoutError("VLM single bbox count æ¨ç†è¶…æ—¶")

    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(15)  # è®¾ç½® 15 ç§’è¶…æ—¶
    try:
        with torch.inference_mode():
            count_output_ids = model.generate(**count_kwargs)
            count_outputs = tokenizer.decode(
                count_output_ids[0, count_kwargs["inputs"].shape[1]:]
            ).strip()
    except TimeoutError:
        print(f"âŒ VLM å¯¹åŒºåŸŸ [{x1}, {y1}, {x2}, {y2}] çš„è®¡æ•°æ¨ç†è¶…æ—¶")
        count_outputs = "0"
    finally:
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        signal.signal(signal.SIGALRM, old_handler) # æ¢å¤ä¿¡å·å¤„ç†
    # --- è¶…æ—¶å¤„ç†ç»“æŸ ---

    count = parse_count_from_text(count_outputs)
    return count


def vlm_xywh_to_xyxy(bbox_xywh):
    """VLM çš„ [x, y, w, h] è½¬ä¸º [x1, y1, x2, y2]"""
    if len(bbox_xywh) < 4:
        return None
    x, y, w, h = bbox_xywh[:4]
    if w <= 0 or h <= 0:
        return None
    return int(x), int(y), int(x + w), int(y + h)


def generate_grid_bboxes(w, h, grid_size=3):
    """ç”Ÿæˆ grid_size x grid_size ç½‘æ ¼ bboxï¼ˆxywh æ ¼å¼ï¼‰"""
    bboxes = []
    cell_w, cell_h = w // grid_size, h // grid_size
    for i in range(grid_size):
        for j in range(grid_size):
            x = j * cell_w
            y = i * cell_h
            w_ = cell_w if j < grid_size - 1 else w - x
            h_ = cell_h if i < grid_size - 1 else h - y
            bboxes.append([x, y, w_, h_])
    return bboxes


def get_detailed_description(object_name_en: str, img_path: str) -> str: # âœ… ç§»é™¤äº† bbox_list_for_desc å‚æ•°
    """
    ä½¿ç”¨ VLM ç”Ÿæˆè‹±æ–‡è¯¦ç»†æè¿°ï¼ˆåŸºäºæ•´å›¾ï¼‰
    object_name_en: "fire" æˆ– "smoke"ï¼ˆè‹±æ–‡ï¼Œç”¨äºæç¤ºï¼‰
    """
    # è·å–å›¾ç‰‡å°ºå¯¸ï¼Œç”¨äºæ„å»ºæ•´å›¾ bbox
    img_temp = Image.open(img_path)
    w, h = img_temp.size
    # ä½¿ç”¨æ•´å›¾ä½œä¸ºæè¿°åŒºåŸŸ
    whole_image_bbox_list = [[0, 0, w, h]]

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {
                    "type": "text",
                    "text": f"Please describe the {object_name_en} in this image in detail. If it does not exist, clearly state that there is no {object_name_en}.",
                },
            ],
            # âœ… ä½¿ç”¨æ•´å›¾ bbox
            "bbox_list": whole_image_bbox_list,
        }
    ]

    kwargs = prepare_inputs(
        model_path,
        model,
        image_processors,
        tokenizer,
        messages,
        max_tokens=4096,
        top_p=0.9,
        temperature=0.7,
        do_sample=True,
    )

    # --- è¶…æ—¶å¤„ç† ---
    def timeout_handler(signum, frame):
        raise TimeoutError("VLM description æ¨ç†è¶…æ—¶")

    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(30)  # è®¾ç½® 30 ç§’è¶…æ—¶
    try:
        with torch.inference_mode():
            output_ids = model.generate(**kwargs)
            outputs = tokenizer.decode(
                output_ids[0, kwargs["inputs"].shape[1]:]
            ).strip()
    except TimeoutError:
        print("âŒ VLM description æ¨ç†è¶…æ—¶ï¼Œè¿”å›é»˜è®¤æè¿°")
        outputs = f"[{object_name_en}æè¿°è¶…æ—¶]"
    finally:
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        signal.signal(signal.SIGALRM, old_handler) # æ¢å¤ä¿¡å·å¤„ç†
    # --- è¶…æ—¶å¤„ç†ç»“æŸ ---

    return outputs


# ================== ä¸»æµç¨‹ ==================
def main():
    all_logs = []

    if not os.path.isdir(image_folder):
        raise NotADirectoryError(f"âŒ å›¾åƒè¾“å…¥ç›®å½•ä¸å­˜åœ¨: {image_folder}")

    # æŒ‰æ–‡ä»¶åæ’åºå¤„ç†
    img_files = sorted(os.listdir(image_folder))

    if not img_files:
        print(f"âš ï¸ è¾“å…¥ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶: {image_folder}")
        return

    for img_filename in img_files:
        if not img_filename.lower().endswith((".png", ".jpg", ".jpeg", ".bmp", ".gif")):
            continue

        img_path = os.path.join(image_folder, img_filename)
        print("\n==============================")
        print(f"å¼€å§‹å¤„ç†å›¾åƒ: {img_path}")

        # Step 1: YOLO æ¨ç†
        yolov5_results, orig_shape = get_yolov5_bboxes(
            yolov5_model, img_path, conf_thresh=0.2
        )
        if orig_shape is None:
            print(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒï¼Œå·²è·³è¿‡: {img_path}")
            continue

        # Step 2: æ„å»ºç»™ VLM çš„ bbox_list
        if yolov5_results:
            print(f"âœ… YOLO æ£€æµ‹åˆ° {len(yolov5_results)} ä¸ªå€™é€‰æ¡†")
            bbox_list = [
                [x1, y1, (x2 - x1), (y2 - y1)]
                for x1, y1, x2, y2, _, _ in yolov5_results
            ]
        else:
            img_temp = Image.open(img_path)
            w, h = img_temp.size
            grid_size = 3
            bbox_list = generate_grid_bboxes(w, h, grid_size=grid_size)
            print(
                f"ğŸ” YOLO æœªæ£€æµ‹åˆ°ä»»ä½•æ¡†ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸º {grid_size}x{grid_size} ç½‘æ ¼ï¼ˆå…± {len(bbox_list)} ä¸ªåŒºåŸŸï¼‰æäº¤ç»™ VLM"
            )

        # Step 3: ä½¿ç”¨ VLM æ£€æµ‹ç«ç„° & çƒŸé›¾ (ä»…åœ¨ YOLO æ— æ¡†æ—¶)
        if not yolov5_results:
            try:
                vlm_fire_bboxes, fire_count = detect_and_count("fire", bbox_list, img_path)
                vlm_smoke_bboxes, smoke_count = detect_and_count("smoke", bbox_list, img_path)
            except Exception as e:
                print(f"âŒ VLM æ¨ç†å‡ºé”™ï¼Œè¯¥å›¾ç‰‡è·³è¿‡ã€‚é”™è¯¯ä¿¡æ¯: {e}")
                continue
        else:
            # YOLO æœ‰æ¡†æ—¶ï¼Œä¸è¿›è¡Œå…¨å±€æ£€æµ‹ï¼Œåªè¿›è¡Œå•æ¡†è®¡æ•°
            vlm_fire_bboxes, vlm_smoke_bboxes = [], []
            # è®¡æ•°åŸºäº YOLO æ¡†çš„æ•°é‡
            fire_count = sum(1 for _, _, _, _, _, cls_id in yolov5_results if cls_id == fire_label)
            smoke_count = sum(1 for _, _, _, _, _, cls_id in yolov5_results if cls_id == smoke_label)

        # Step 4: èåˆä¸ºç»˜å›¾ç”¨çš„æ¡†
        fused_bboxes = []

        if yolov5_results:
            # ä½¿ç”¨ YOLO çš„ç±»åˆ«å’Œç½®ä¿¡åº¦ï¼Œå¯¹å¤–æ˜¾ç¤ºè‹±æ–‡
            for x1, y1, x2, y2, conf, cls_id in yolov5_results:
                if cls_id == fire_label:
                    label_en = "Fire"
                    object_name_en = "fire"
                elif cls_id == smoke_label:
                    label_en = "Smoke"
                    object_name_en = "smoke"
                else:
                    label_en = None
                    object_name_en = None

                if label_en:
                    # âœ… æ–°å¢ï¼šå¯¹å•ä¸ª YOLO æ¡†è¿›è¡Œ VLM è®¡æ•°éªŒè¯
                    vlm_count_in_bbox = detect_count_in_single_bbox(object_name_en, x1, y1, x2, y2, img_path)
                    # å¦‚æœ VLM è®¡æ•° > 1ï¼Œåˆ™æå‡ç½®ä¿¡åº¦
                    final_conf = 0.9 if vlm_count_in_bbox > 1 else conf
                    print(f"  - YOLO æ¡† [{x1}, {y1}, {x2}, {y2}] ({label_en})ï¼ŒVLM è®¡æ•°: {vlm_count_in_bbox}ï¼Œç½®ä¿¡åº¦: {conf:.2f} -> {final_conf:.2f}")

                    fused_bboxes.append(
                        {
                            "x1": int(x1),
                            "y1": int(y1),
                            "x2": int(x2),
                            "y2": int(y2),
                            "conf": final_conf,
                            "label": label_en,
                        }
                    )
        elif fire_count > 0 or smoke_count > 0:
            # ç½‘æ ¼æ¨¡å¼ï¼šä»…ä½¿ç”¨ VLM æ£€æµ‹ç»“æœï¼Œç½®ä¿¡åº¦ç»Ÿä¸€ä¸º 0.5
            for bbox in vlm_fire_bboxes:
                xyxy = vlm_xywh_to_xyxy(bbox)
                if xyxy:
                    fused_bboxes.append(
                        {
                            "x1": xyxy[0],
                            "y1": xyxy[1],
                            "x2": xyxy[2],
                            "y2": xyxy[3],
                            "conf": 0.5,
                            "label": "Fire",
                        }
                    )
            for bbox in vlm_smoke_bboxes:
                xyxy = vlm_xywh_to_xyxy(bbox)
                if xyxy:
                    fused_bboxes.append(
                        {
                            "x1": xyxy[0],
                            "y1": xyxy[1],
                            "x2": xyxy[2],
                            "y2": xyxy[3],
                            "conf": 0.5,
                            "label": "Smoke",
                        }
                    )

        # Step 5: ç»˜åˆ¶æ¡†å¹¶ä¿å­˜ç»“æœå›¾
        image = Image.open(img_path).convert("RGB")
        img_np = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

        for box in fused_bboxes:
            x1, y1, x2, y2 = box["x1"], box["y1"], box["x2"], box["y2"]
            conf, label_en = float(box["conf"]), box["label"]

            # ç«ç„°: ç»¿è‰², çƒŸé›¾: çº¢è‰²
            color = (0, 255, 0) if label_en == "Fire" else (0, 0, 255)

            cv2.rectangle(img_np, (x1, y1), (x2, y2), color, 2)
            cv2.putText(
                img_np,
                f"{label_en} {conf:.2f}",
                (x1, max(y1 - 10, 0)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8,
                color,
                2,
            )

        output_path = os.path.join(
            output_folder, os.path.splitext(img_filename)[0] + "_result.jpg"
        )
        cv2.imwrite(output_path, img_np)

        # Step 6: ç”Ÿæˆç«ç„°/çƒŸé›¾çš„è‹±æ–‡è¯¦ç»†æè¿°ï¼ˆåŸºäºæ•´å›¾ï¼‰
        # âœ… è°ƒç”¨æ–¹å¼ç®€åŒ–ï¼Œä¸å†ä¼ å…¥ bbox_list
        try:
            fire_desc = get_detailed_description("fire", img_path)
        except Exception as e:
            fire_desc = f"[Fire description failed: {e}]"

        try:
            smoke_desc = get_detailed_description("smoke", img_path)
        except Exception as e:
            smoke_desc = f"[Smoke description failed: {e}]"

        # Step 7: æ„å»º & è¾“å‡ºæ—¥å¿—ï¼ˆåŒ…å«è¯¦ç»†æè¿°ï¼‰
        # âœ… ç§»é™¤ desc_path å®šä¹‰
        # desc_path = os.path.join(output_folder, os.path.splitext(img_filename)[0] + "_description.txt")

        log_lines = [
            "==============================",
            f"Processing image path: {img_path}",
            f"Fire count (detection result): {fire_count}",
            f"Smoke count (detection result): {smoke_count}",
            # âœ… ç›´æ¥åœ¨æ—¥å¿—ä¸­åŒ…å«è¯¦ç»†æè¿°
            f"Fire Description: {fire_desc}",
            f"Smoke Description: {smoke_desc}",
            f"Detection visualization result saved to: {output_path}",
            # âœ… ç§»é™¤ "æ–‡å­—æè¿°æ–‡ä»¶å·²ä¿å­˜è‡³..." è¿™ä¸€è¡Œ
            # f"Text description file saved to: {desc_path}",
        ]

        if not fused_bboxes and fire_count == 0 and smoke_count == 0:
            log_lines.append("No fire or smoke detected in this image.")

        current_log = "\n".join(log_lines)
        all_logs.append(current_log)
        print(current_log)

        # Step 8: ç§»é™¤ä¿å­˜æè¿°æ–‡ä»¶çš„é€»è¾‘
        # with open(desc_path, "w", encoding="utf-8") as f:
        #     f.write(f"ç«ç„°è¯¦ç»†æè¿°:\n{fire_desc}\n\n")
        #     f.write(f"çƒŸé›¾è¯¦ç»†æè¿°:\n{smoke_desc}\n")

    # ================== æ±‡æ€»æ—¥å¿— ==================
    print("\n\n" + "=" * 70)
    print("âœ… æ‰€æœ‰å›¾åƒå¤„ç†å®Œæˆï¼å®Œæ•´æ±‡æ€»æ—¥å¿—å¦‚ä¸‹ï¼š")
    print("=" * 70)
    full_log = "\n\n".join(all_logs)
    print(full_log)

    total_log_path = os.path.join(output_folder, "total_processing_log.txt")
    with open(total_log_path, "w", encoding="utf-8") as f:
        f.write(full_log)
    print(f"\nğŸ“„ å®Œæ•´æ±‡æ€»æ—¥å¿—å·²ä¿å­˜è‡³: {total_log_path}")


if __name__ == "__main__":
    main()