"""
å®Œæ•´æœ¬åœ°ç‰ˆæ¨ç†è„šæœ¬ï¼ˆæ—  torch.hub / ultralytics ä¾èµ–ï¼‰
åŠŸèƒ½ï¼šèåˆ YOLOv5ã€UPN å’Œ VLM-FO1ï¼Œå®ç°ç«ç¾/çƒŸé›¾çš„é«˜çº§æ£€æµ‹ã€è®¡æ•°å’Œæè¿°ã€‚
æµç¨‹ï¼šYOLOv5 -> UPN (Fallback) -> VLM-FO1 (Detection/Counting) -> Fusion -> Descriptionã€‚
"""
import os
import re
import sys
import torch
import cv2
import numpy as np
from PIL import Image
from pathlib import Path

# ================== é…ç½® YOLOv5 æœ¬åœ°è·¯å¾„å¹¶æ ¡éªŒ ==================
# å¿…é¡»æŒ‡å‘åŒ…å« models/ çš„é‚£ä¸€å±‚ç›®å½•
YOLOV5_LOCAL_DIR = "/root/autodl-tmp/yolov5/yolov5-master"

if not os.path.exists(YOLOV5_LOCAL_DIR):
    raise FileNotFoundError(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {YOLOV5_LOCAL_DIR}")

models_path = os.path.join(YOLOV5_LOCAL_DIR, "models")
if not os.path.exists(models_path):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° models æ–‡ä»¶å¤¹ï¼è¯·æ£€æŸ¥: {models_path}")

# å°† YOLOv5 è·¯å¾„æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…¶æ¨¡å—
sys.path.insert(0, YOLOV5_LOCAL_DIR)
print(f"âœ… å·²æˆåŠŸæ·»åŠ  YOLOv5 è·¯å¾„: {YOLOV5_LOCAL_DIR}")

# ================== å¯¼å…¥æœ¬åœ° YOLOv5 æ¨¡å— ==================
from models.experimental import attempt_load                 # ç”¨äºåŠ è½½æœ¬åœ°çš„æ¨¡å‹æƒé‡
from utils.general import non_max_suppression, scale_boxes  # ç”¨äºéæå¤§å€¼æŠ‘åˆ¶å’Œåæ ‡ç¼©æ”¾
from utils.augmentations import letterbox                   # ç”¨äºå›¾åƒé¢„å¤„ç†ï¼ˆLetterboxï¼‰

# ================== å¯¼å…¥ VLM-FO1 æ¨¡å— ==================
from vlm_fo1.model.builder import load_pretrained_model
from vlm_fo1.mm_utils import prepare_inputs, extract_predictions_to_bboxes
from vlm_fo1.task_templates import OD_template

# ================== å¯¼å…¥ UPN æ£€æµ‹å™¨å°è£… ==================
try:
    from detect_tools.upn.inference_wrapper import UPNWrapper # å°è¯•å¯¼å…¥ UPNWrapper
    has_upn = True
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œç¦ç”¨ UPN åŠŸèƒ½
    print("âš ï¸ æœªæ‰¾åˆ° detect_tools.upn.inference_wrapper.UPNWrapperï¼ŒUPN æ£€æµ‹å°†è¢«ç¦ç”¨ï¼Œåªä½¿ç”¨ YOLO/æ•´å›¾ bboxã€‚")
    has_upn = False

# ================== é…ç½®è·¯å¾„ ==================
image_folder = "/root/autodl-tmp/datasets_input/images"    # è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
output_folder = "/root/autodl-tmp/datasets_output"      # è¾“å‡ºç»“æœæ–‡ä»¶å¤¹è·¯å¾„
model_path = '/root/autodl-tmp/resources/resources/VLM-FO1_Qwen2.5-VL-3B-v01' # VLM-FO1 æ¨¡å‹è·¯å¾„
wheel_path = '/root/autodl-tmp/resources/resources/flash_attn-2.8.0+cu124torch2.6-cp311-cp311-linux_x86_64.whl'
yolov5_weights_path = "/root/autodl-tmp/exp8best/weights/best.pt" # YOLOv5 æƒé‡è·¯å¾„
upn_ckpt_path = "/root/autodl-tmp/resources/resources/upn_large.pth" # UPN checkpoint è·¯å¾„

os.makedirs(output_folder, exist_ok=True) # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹

# ================== å®‰è£… flash-attnï¼ˆå¯é€‰ï¼‰ ==================
if os.path.exists(wheel_path):
    os.system(f"pip install -q {wheel_path}") # é™é»˜å®‰è£… flash-attn wheel æ–‡ä»¶

# ================== åŠ è½½ VLM-FO1 æ¨¡å‹ ==================
print("ğŸ”¥ åŠ è½½ VLM-FO1 æ¨¡å‹...")
tokenizer, model, image_processors = load_pretrained_model(
    model_path,
    load_8bit=False,
    load_4bit=False,
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# ================== åŠ è½½æœ¬åœ° YOLOv5 æ¨¡å‹ ==================
print("ğŸ”¥ åŠ è½½æœ¬åœ° YOLOv5 æ¨¡å‹ (v7.0)...")
device = "cuda" if torch.cuda.is_available() else "cpu" # ç¡®å®šè¿è¡Œè®¾å¤‡
yolov5_model = attempt_load(yolov5_weights_path, device=device)
yolov5_model.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼

# ================== ï¼ˆå¯é€‰ï¼‰åŠ è½½ UPN æ£€æµ‹å™¨ ==================
upn_detector = None
if has_upn and os.path.exists(upn_ckpt_path):
    try:
        print("ğŸ”¥ åŠ è½½ UPN æ£€æµ‹å™¨ï¼Œç”¨äº YOLO æ— æ£€æµ‹æ¡†æ—¶çš„è‡ªåŠ¨æ£€æµ‹...")
        upn_detector = UPNWrapper(ckpt_path=upn_ckpt_path) # åˆå§‹åŒ– UPN æ£€æµ‹å™¨
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ UPN æ£€æµ‹å™¨å¤±è´¥ï¼Œå°†ä¸ä¼šä½¿ç”¨ UPNã€‚é”™è¯¯: {e}")
        upn_detector = None
else:
    if has_upn:
        print(f"âš ï¸ æœªæ‰¾åˆ° UPN checkpoint æ–‡ä»¶: {upn_ckpt_path}ï¼Œå°†ä¸ä¼šä½¿ç”¨ UPNã€‚")

# ç±»åˆ«æ ‡ç­¾ï¼ˆå¿…é¡»ä¸ YOLOv5 è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
fire_label = 0
smoke_label = 1

# ================== å·¥å…·å‡½æ•° ==================
def parse_count_from_text(text: str) -> int:
    """ä» VLM çš„è‡ªç„¶è¯­è¨€å›ç­”ä¸­æå–æ•°é‡ï¼ˆæ”¯æŒè‹±æ–‡æ•°å­—å’Œé˜¿æ‹‰ä¼¯æ•°å­—ï¼‰"""
    text_lower = text.lower().strip()

    # æ˜ç¡®æ— ç›®æ ‡çš„æƒ…å†µ
    if any(word in text_lower for word in
           ["no", "none", "not", "zero", "not visible", "not detected", "no fire", "no smoke", "not any", "not present", "absence", "missing",
            "æ²¡æœ‰", "é›¶ä¸ª", "æœªå‘ç°"]):
        return 0

    # è‹±æ–‡æ•°å­—æ˜ å°„ï¼ˆè¦†ç›–å¸¸è§å€¼ï¼‰
    word_to_num = {
        "one": 1, "two": 2, "three": 3, "four": 4, "five": 5,
        "six": 6, "seven": 7, "eight": 8, "nine": 9, "ten": 10,"eleven": 11, "twelve": 12,
        "a single": 1,"dozens": 12,"hundreds": 100,
        "a couple": 2,"multiple": 2, "several": 3, "many": 5, "a lot": 5
    }

    for word, num in word_to_num.items():
        if word in text_lower:
            return num

    # å°è¯•æå–é˜¿æ‹‰ä¼¯æ•°å­—
    numbers = re.findall(r'\d+', text)
    if numbers:
        return int(numbers[0])

    # é»˜è®¤ï¼šå¦‚æœæåˆ°å¯¹è±¡ä½†æ²¡ç»™æ•°é‡ï¼Œè‡³å°‘ç®— 1
    if any(obj in text_lower for obj in ["fire", "smoke", "flame", "burn", "smoke", "çƒŸ", "ç«"]):
        return 1

    return 0


def get_yolov5_bboxes(model, img_path, conf_thresh=0.3):
    """
    ä½¿ç”¨æœ¬åœ° YOLOv5 æ¨ç†ã€‚
    è¿”å›: [x1, y1, x2, y2, conf, cls] åˆ—è¡¨ å’Œ åŸå§‹å›¾åƒ shape
    """
    img0 = cv2.imread(img_path)
    if img0 is None:
        return [], None

    # é¢„å¤„ç†ï¼šLetterbox ç¼©æ”¾ï¼Œè°ƒæ•´ç»´åº¦ (HWC -> CHW)ï¼Œå½’ä¸€åŒ–
    img = letterbox(img0, 640, stride=32, auto=True)[0]
    img = img.transpose((2, 0, 1))
    img = np.ascontiguousarray(img)
    img = torch.from_numpy(img).to(device).float() / 255.0
    if img.ndimension() == 3:
        img = img.unsqueeze(0) # å¢åŠ  Batch ç»´åº¦

    # æ¨ç† + NMS
    with torch.no_grad():
        pred = model(img)[0]
    pred = non_max_suppression(pred, conf_thresh, 0.45, classes=None, agnostic=False)

    full_detections = [] # å­˜å‚¨å®Œæ•´çš„ YOLO è¾“å‡ºï¼š[x1, y1, x2, y2, conf, cls]
    for det in pred:
        if len(det):
            # å°†åæ ‡ä» 640x640 ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå°ºå¯¸
            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()
            for *xyxy, conf, cls in det:
                if float(conf) > conf_thresh:
                    x1, y1, x2, y2 = map(int, xyxy)
                    # ä¿å­˜å®Œæ•´ä¿¡æ¯
                    full_detections.append([x1, y1, x2, y2, float(conf), int(cls)])

    return full_detections, img0.shape # è¿”å›å®Œæ•´çš„æ£€æµ‹ç»“æœå’Œå›¾åƒå°ºå¯¸


def detect_and_count(object_name, bbox_list, img_path):
    """
    ä½¿ç”¨ VLM-FO1 åœ¨ç»™å®šçš„ Proposals (bbox_list) ä¸Šè¿›è¡Œç›®æ ‡æ£€æµ‹å’Œè®¡æ•°ã€‚
    è¿”å›: VLM æ£€æµ‹åˆ°çš„ç›®æ ‡æ¡† (xywh æ ¼å¼) å’Œè®¡æ•°ã€‚
    """
    # ---------- 1. ç›®æ ‡æ£€æµ‹ä»»åŠ¡ ----------
    detect_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": OD_template.format(object_name)}, # ä½¿ç”¨ç›®æ ‡æ£€æµ‹æ¨¡æ¿
            ],
            "bbox_list": bbox_list, # ä¼ å…¥ Proposals (xywh)
        }
    ]

    # ---------- 2. ç›®æ ‡è®¡æ•°ä»»åŠ¡ ----------
    count_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": f"How many {object_name} are there in this image?"}, # è®¡æ•° Prompt
            ],
            "bbox_list": bbox_list,
        }
    ]

    # æ‰§è¡Œæ£€æµ‹æ¨ç†
    detect_kwargs = prepare_inputs(model_path, model, image_processors, tokenizer, detect_messages, max_tokens=4096, top_p=0.05, temperature=0.0, do_sample=False)
    with torch.inference_mode():
        detect_output_ids = model.generate(**detect_kwargs)
        detect_outputs = tokenizer.decode(detect_output_ids[0, detect_kwargs['inputs'].shape[1]:]).strip()

    # ä» VLM çš„è¾“å‡ºä¸­æå–ç›®æ ‡æ¡†
    label_to_bboxes = extract_predictions_to_bboxes(detect_outputs, bbox_list)
    bboxes = label_to_bboxes.get(object_name.lower(), []) # VLM è¯†åˆ«å‡ºçš„ç›®æ ‡æ¡†ï¼Œæ ¼å¼ä¸º xywh

    # æ‰§è¡Œè®¡æ•°æ¨ç†
    count_kwargs = prepare_inputs(model_path, model, image_processors, tokenizer, count_messages, max_tokens=4096, top_p=0.05, temperature=0.0, do_sample=False)
    with torch.inference_mode():
        count_output_ids = model.generate(**count_kwargs)
        count_outputs = tokenizer.decode(count_output_ids[0, count_kwargs['inputs'].shape[1]:]).strip()

    # ä»æ–‡æœ¬ä¸­è§£ææ•°é‡
    count = parse_count_from_text(count_outputs)

    return bboxes, count


def vlm_xywh_to_xyxy(bbox_xywh):
    """å°† VLM è¾“å‡ºçš„ [x, y, w, h] æ ¼å¼è½¬æ¢ä¸º [x1, y1, x2, y2]"""
    if len(bbox_xywh) < 4:
        return None
    x, y, w, h = bbox_xywh[:4]
    if w <= 0 or h <= 0:
        return None
    return int(x), int(y), int(x + w), int(y + h)


def compute_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„ IoU"""
    x1_min, y1_min, x1_max, y1_max = box1
    x2_min, y2_min, x2_max, y2_max = box2
    # è®¡ç®—äº¤é›†åæ ‡
    inter_x1 = max(x1_min, x2_min)
    inter_y1 = max(y1_min, y2_min)
    inter_x2 = min(x1_max, x2_max)
    inter_y2 = min(y1_max, y2_max)
    # è®¡ç®—äº¤é›†é¢ç§¯
    inter_w = max(0, inter_x2 - inter_x1)
    inter_h = max(0, inter_y2 - inter_y1)
    inter_area = inter_w * inter_h
    # è®¡ç®—å¹¶é›†é¢ç§¯
    area1 = (x1_max - x1_min) * (y1_max - y1_min)
    area2 = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = area1 + area2 - inter_area
    return inter_area / union_area if union_area > 0 else 0.0


def fuse_results(yolov5_dets, vlm_fire_bboxes, vlm_smoke_bboxes, fire_count, smoke_count, iou_threshold=0.3):
    """
    YOLO + VLM ç»“æœèåˆé€»è¾‘ã€‚
    yolov5_dets æ ¼å¼: [x1, y1, x2, y2, conf, cls]
    vlm_bboxes æ ¼å¼: [x, y, w, h]
    """
    fused_bboxes = []

    def process_vlm_bboxes(vlm_dets):
        """å°† VLM çš„ xywh æ¡†è½¬æ¢ä¸º xyxyï¼Œå¹¶èµ‹äºˆé»˜è®¤é«˜ç½®ä¿¡åº¦ (0.9)"""
        processed = []
        for bbox_xywh in vlm_dets:
            xyxy = vlm_xywh_to_xyxy(bbox_xywh)
            if xyxy is None or xyxy[0] >= xyxy[2] or xyxy[1] >= xyxy[3]:
                continue
            processed.append((xyxy[0], xyxy[1], xyxy[2], xyxy[3], 0.9)) # é»˜è®¤ç½®ä¿¡åº¦ 0.9
        return processed

    vlm_fire_xyxy = process_vlm_bboxes(vlm_fire_bboxes) if fire_count > 0 else []
    vlm_smoke_xyxy = process_vlm_bboxes(vlm_smoke_bboxes) if smoke_count > 0 else []

    # æƒ…å†µ1ï¼šèåˆ YOLOv5 æ£€æµ‹ç»“æœ (ä»¥ YOLO ç»“æœä¸ºä¸»)
    for x1, y1, x2, y2, conf, cls in yolov5_dets:
        if conf <= 0.15: # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„ YOLO æ¡†
            continue
        cls_id = int(cls)
        yolo_box = (x1, y1, x2, y2)
        label_name = None
        should_promote = False # æ ‡è®°æ˜¯å¦åº”è¢« VLM ç¡®è®¤è€Œæå‡ç½®ä¿¡åº¦

        # æ£€æŸ¥æ˜¯å¦ä¸ VLM çš„ Fire æ¡†é‡åˆ
        if cls_id == fire_label and fire_count > 0:
            label_name = "Fire"
            for xb in vlm_fire_xyxy:
                if compute_iou(yolo_box, xb[:4]) > iou_threshold:
                    should_promote = True
                    break
        # æ£€æŸ¥æ˜¯å¦ä¸ VLM çš„ Smoke æ¡†é‡åˆ
        elif cls_id == smoke_label and smoke_count > 0:
            label_name = "Smoke"
            for xb in vlm_smoke_xyxy:
                if compute_iou(yolo_box, xb[:4]) > iou_threshold:
                    should_promote = True
                    break
        else:
            continue

        if label_name is None:
            continue

        # å¦‚æœ VLM ç¡®è®¤ï¼Œå°†ç½®ä¿¡åº¦æå‡åˆ° 0.9
        final_conf = 0.9 if should_promote else conf
        fused_bboxes.append({
            "x1": x1, "y1": y1, "x2": x2, "y2": y2,
            "conf": final_conf, "label": label_name
        })

    # æƒ…å†µ2ï¼šå›é€€åˆ° VLM ç»“æœ (å¦‚æœ YOLO æœªæ£€æµ‹åˆ°æˆ–æ£€æµ‹ç»“æœè¢«è¿‡æ»¤å®Œ)
    if not fused_bboxes and (fire_count > 0 or smoke_count > 0):
        for x1, y1, x2, y2, conf in vlm_fire_xyxy:
            # å°† VLM è¯†åˆ«å‡ºçš„ Fire æ¡†åŠ å…¥ç»“æœ (ä½¿ç”¨ VLM é»˜è®¤çš„ 0.9 ç½®ä¿¡åº¦)
            fused_bboxes.append({"x1": x1, "y1": y1, "x2": x2, "y2": y2, "conf": conf, "label": "Fire"})
        for x1, y1, x2, y2, conf in vlm_smoke_xyxy:
            # å°† VLM è¯†åˆ«å‡ºçš„ Smoke æ¡†åŠ å…¥ç»“æœ
            fused_bboxes.append({"x1": x1, "y1": y1, "x2": x2, "y2": y2, "conf": conf, "label": "Smoke"})

    return fused_bboxes


def get_detailed_description(object_name, img_path):
    """
    ä½¿ç”¨ VLM-FO1 ç”Ÿæˆè¯¦ç»†æè¿°ã€‚ä½¿ç”¨æ•´å›¾ bbox ä½œä¸º Proposalã€‚
    æ— è®ºæ˜¯å¦æ£€æµ‹åˆ°ç›®æ ‡ï¼Œéƒ½ä¼šå°è¯•æè¿°ã€‚
    """
    img_temp = Image.open(img_path)
    w, h = img_temp.size
    bbox_list = [[0, 0, w, h]] # ä½¿ç”¨æ•´å›¾ä½œä¸ºå”¯ä¸€çš„ Proposal [x, y, width, height]

    description_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": f"Describe the {object_name} in this image."}, # æè¿° Prompt
            ],
            "bbox_list": bbox_list,
        }
    ]
    # ä½¿ç”¨ Top-p å’Œ Temperature è¿›è¡Œé‡‡æ ·ï¼Œä»¥è·å¾—æ›´å…·åˆ›é€ æ€§çš„æè¿°
    kwargs = prepare_inputs(model_path, model, image_processors, tokenizer, description_messages, max_tokens=4096, top_p=0.9, temperature=0.7, do_sample=True)
    with torch.inference_mode():
        output_ids = model.generate(**kwargs)
        outputs = tokenizer.decode(output_ids[0, kwargs['inputs'].shape[1]:]).strip()
    return outputs


# ================== ä¸»æµç¨‹ ==================
def main():
    for img_filename in os.listdir(image_folder):
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºå›¾ç‰‡
        if not img_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
            continue

        img_path = os.path.join(image_folder, img_filename)
        print(f"\n==============================")
        print(f"å¤„ç†å›¾åƒ: {img_path}")

        # --- Step 1: YOLOv5 æ¨ç† ---
        # yolov5_full_dets æ ¼å¼: [x1, y1, x2, y2, conf, cls]
        yolov5_full_dets, orig_shape = get_yolov5_bboxes(yolov5_model, img_path, conf_thresh=0.3)
        if orig_shape is None:
            print(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒï¼Œè·³è¿‡: {img_path}")
            continue

        # --- Step 2: æ„å»º bbox_list (VLM çš„ Proposals è¾“å…¥) ---
        bbox_list = []

        if yolov5_full_dets:
            # ç­–ç•¥ 1: æœ‰ YOLO æ£€æµ‹æ¡† â†’ ä½¿ç”¨ YOLO æ¡†ä½œä¸º Proposal
            print(f"âœ… YOLO æ£€æµ‹åˆ° {len(yolov5_full_dets)} ä¸ªæ¡†ï¼Œç”¨äº FO1 æ¨ç†ã€‚")
            # å°† YOLO çš„ xyxy æ ¼å¼è½¬æ¢ä¸º VLM æ‰€éœ€çš„ xywh æ ¼å¼
            bbox_list = [[d[0], d[1], d[2] - d[0], d[3] - d[1]] for d in yolov5_full_dets]

        elif upn_detector is not None:
            # ç­–ç•¥ 2: YOLO æ— æ¡†ä¸” UPN å¯ç”¨ â†’ ä½¿ç”¨ UPN ç”Ÿæˆ Proposal
            try:
                print("ğŸ” YOLO æ— æ£€æµ‹æ¡†ï¼Œä½¿ç”¨ UPN æ£€æµ‹å™¨ç”Ÿæˆ proposals ...")
                pil_img = Image.open(img_path).convert("RGB")
                # UPN æ¨ç†å¹¶è¿‡æ»¤
                upn_raw = upn_detector.inference(pil_img, prompt_type="fine_grained_prompt")
                upn_filtered = upn_detector.filter(upn_raw, min_score=0.4, nms_value=0.8)

                upn_boxes_xyxy = upn_filtered.get("boxes", []) if isinstance(upn_filtered, dict) else []
                print(f"âœ… UPN æ£€æµ‹åˆ° {len(upn_boxes_xyxy)} ä¸ª proposalsã€‚")

                if len(upn_boxes_xyxy) > 0:
                    # å°† UPN çš„ xyxy æ ¼å¼è½¬æ¢ä¸º VLM æ‰€éœ€çš„ xywh æ ¼å¼
                    bbox_list = [[float(x1), float(y1), float(x2 - x1), float(y2 - y1)]
                                 for x1, y1, x2, y2 in upn_boxes_xyxy]

            except Exception as e:
                print(f"âš ï¸ UPN æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œå°†é€€å›æ•´å›¾ bboxã€‚é”™è¯¯: {e}")
                bbox_list = []

        if not bbox_list:
            # ç­–ç•¥ 3: æ— æœ‰æ•ˆ Proposals â†’ ä½¿ç”¨æ•´å›¾ä½œä¸º Proposal
            h, w = orig_shape[:2]
            print("âš ï¸ YOLO å’Œ UPN å‡æ— æœ‰æ•ˆæ£€æµ‹æ¡†ï¼Œä½¿ç”¨æ•´å›¾ä½œä¸ºå•ä¸€ bboxã€‚")
            bbox_list = [[0, 0, w, h]] # [x, y, width, height]

        # --- Step 3: è°ƒç”¨ VLM-FO1 è¿›è¡Œ fire / smoke æ£€æµ‹ä¸è®¡æ•° ---
        try:
            # vlm_fire_bboxes æ ¼å¼ä¸º VLM è¾“å‡ºçš„ xywh
            vlm_fire_bboxes, fire_count = detect_and_count("fire", bbox_list, img_path)
            vlm_smoke_bboxes, smoke_count = detect_and_count("smoke", bbox_list, img_path)
        except Exception as e:
            print(f"âŒ VLM æ¨ç†å‡ºé”™ï¼ˆè·³è¿‡æ­¤å›¾ï¼‰: {e}")
            continue

        # --- Step 4: å‡†å¤‡ YOLO ç»“æœç”¨äºèåˆ ---
        # ä½¿ç”¨ Step 1 ä¸­è·å–çš„å®Œæ•´ YOLO æ£€æµ‹ç»“æœ
        yolov5_dets = yolov5_full_dets

        # --- Step 5: èåˆ YOLO + VLM ç»“æœ ---
        # fused_bboxes æ ¼å¼: å­—å…¸åˆ—è¡¨ [{"x1": x, "y1": y, "x2": x, "y2": y, "conf": c, "label": l}, ...]
        fused_bboxes = fuse_results(
            yolov5_dets,
            vlm_fire_bboxes,
            vlm_smoke_bboxes,
            fire_count,
            smoke_count,
            iou_threshold=0.3
        )

        # --- Step 6: ç»˜å›¾å¹¶ä¿å­˜ ---
        image = Image.open(img_path).convert("RGB")
        img_np = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR) # PIL Image è½¬ OpenCV æ ¼å¼

        for box in fused_bboxes:
            # æå–èåˆåçš„è¾¹ç•Œæ¡†ä¿¡æ¯
            x1 = int(box["x1"])
            y1 = int(box["y1"])
            x2 = int(box["x2"])
            y2 = int(box["y2"])
            conf = float(box["conf"])
            label = box["label"]

            # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
            color = (0, 255, 0) if label == "Fire" else (0, 0, 255) # Fire ç”¨ç»¿è‰²ï¼ŒSmoke ç”¨è“è‰²
            cv2.rectangle(img_np, (x1, y1), (x2, y2), color, 2)
            cv2.putText(
                img_np,
                f"{label} {conf:.2f}",
                (x1, max(y1 - 10, 0)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8,
                color,
                2
            )

        output_filename = os.path.splitext(img_filename)[0] + "_result.jpg"
        output_path = os.path.join(output_folder, output_filename)
        cv2.imwrite(output_path, img_np) # ä¿å­˜ç»˜åˆ¶ç»“æœ

        # --- Step 7: ç”Ÿæˆè¯¦ç»†æè¿° ---
        try:
            detailed_description_fire = get_detailed_description("fire", img_path)
        except Exception as e:
            detailed_description_fire = f"[Fire æè¿°å¤±è´¥: {e}]"

        try:
            detailed_description_smoke = get_detailed_description("smoke", img_path)
        except Exception as e:
            detailed_description_smoke = f"[Smoke æè¿°å¤±è´¥: {e}]"

        print(f"Fire Description: {detailed_description_fire}")
        print(f"Smoke Description: {detailed_description_smoke}")

        # ä¿å­˜æè¿°åˆ°æ–‡æœ¬æ–‡ä»¶
        description_filename = os.path.splitext(img_filename)[0] + "_description.txt"
        description_path = os.path.join(output_folder, description_filename)
        with open(description_path, "w", encoding="utf-8") as f:
            f.write(f"Fire Description: {detailed_description_fire}\n")
            f.write(f"Smoke Description: {detailed_description_smoke}\n")

        print(f"ç«çš„æ•°é‡: {fire_count}")
        print(f"çƒŸçš„æ•°é‡: {smoke_count}")
        print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"æè¿°æ–‡ä»¶å·²ä¿å­˜åˆ°: {description_path}")

        if len(fused_bboxes) == 0 and fire_count == 0 and smoke_count == 0:
            print("æœªæ£€æµ‹åˆ°ä»»ä½• Fire æˆ– Smokeã€‚")

if __name__ == '__main__':
    main()
