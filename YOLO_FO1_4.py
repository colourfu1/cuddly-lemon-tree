# -*- coding: utf- -*-
"""
å®Œæ•´æœ¬åœ°ç‰ˆæ¨ç†è„šæœ¬ï¼ˆæ—  torch.hub / ultralytics ä¾èµ–ï¼‰
YOLOv5 ä½¿ç”¨æœ¬åœ° v7.0 ä»£ç  + è‡ªå®šä¹‰ best.pt
åœ¨ YOLO æ²¡æœ‰æ£€æµ‹æ¡†æ—¶ï¼Œä¼šè°ƒç”¨ VLM-FO1 è‡ªå¸¦çš„ UPN æ£€æµ‹å™¨ç”Ÿæˆ proposalsã€‚
æ— è®ºæ˜¯å¦æœ‰æ£€æµ‹æ¡†ã€æ˜¯å¦æ£€æµ‹åˆ° fire/smokeï¼Œéƒ½ä¼šç”Ÿæˆè¯¦ç»†æè¿°ã€‚
"""

import os
import re
import sys

# ================== é…ç½® YOLOv5 æœ¬åœ°è·¯å¾„å¹¶æ ¡éªŒ ==================
YOLOV5_LOCAL_DIR = "/root/autodl-tmp/yolov5/yolov5-master"  # å¿…é¡»æŒ‡å‘åŒ…å« models/ çš„é‚£ä¸€å±‚

if not os.path.exists(YOLOV5_LOCAL_DIR):
    raise FileNotFoundError(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {YOLOV5_LOCAL_DIR}")

models_path = os.path.join(YOLOV5_LOCAL_DIR, "models")
if not os.path.exists(models_path):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° models æ–‡ä»¶å¤¹ï¼è¯·æ£€æŸ¥: {models_path}")

sys.path.insert(0, YOLOV5_LOCAL_DIR)
print(f"âœ… å·²æˆåŠŸæ·»åŠ  YOLOv5 è·¯å¾„: {YOLOV5_LOCAL_DIR}")

import torch
import cv2
import numpy as np
from PIL import Image
from pathlib import Path

# å†æ¬¡æ·»åŠ ï¼ˆä½ åŸæ¥çš„åšæ³•ï¼‰ï¼ŒæŒ‡å‘ YOLOv5 ä¸»ç›®å½•
YOLOV5_LOCAL_DIR = "/root/autodl-tmp/yolov5"
sys.path.insert(0, YOLOV5_LOCAL_DIR)

# ================== å¯¼å…¥æœ¬åœ° YOLOv5 æ¨¡å— ==================
from models.experimental import attempt_load
from utils.general import non_max_suppression, scale_boxes
from utils.augmentations import letterbox

# ================== å¯¼å…¥ VLM-FO1 æ¨¡å— ==================
from vlm_fo1.model.builder import load_pretrained_model
from vlm_fo1.mm_utils import prepare_inputs, draw_bboxes_and_save, extract_predictions_to_bboxes
from vlm_fo1.task_templates import OD_template

# ================== å¯¼å…¥ UPN æ£€æµ‹å™¨å°è£… ==================
# å‚è§å®˜æ–¹æ–‡æ¡£ï¼šUPNWrapper æä¾› inference() å’Œ filter() æ¥å£([DeepWiki](https://deepwiki.com/om-ai-lab/VLM-FO1/2.2-quick-start-guide))
try:
    from detect_tools.upn.inference_wrapper import UPNWrapper
    has_upn = True
except ImportError:
    print("âš ï¸ æœªæ‰¾åˆ° detect_tools.upn.inference_wrapper.UPNWrapperï¼ŒUPN æ£€æµ‹å°†è¢«ç¦ç”¨ï¼Œåªä½¿ç”¨ YOLO/æ•´å›¾ bboxã€‚")
    has_upn = False

# ================== é…ç½®è·¯å¾„ ==================
image_folder = "/root/autodl-tmp/datasets_input/images"       # è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
output_folder = "/root/autodl-tmp/datasets_output"            # è¾“å‡ºç»“æœæ–‡ä»¶å¤¹è·¯å¾„
model_path = '/root/autodl-tmp/resources/resources/VLM-FO1_Qwen2.5-VL-3B-v01'  # VLM-FO1 æ¨¡å‹è·¯å¾„
wheel_path = '/root/autodl-tmp/resources/resources/flash_attn-2.8.0+cu124torch2.6-cp311-cp311-linux_x86_64.whl'
yolov5_weights_path = "/root/autodl-tmp/exp8best/weights/best.pt"  # ä½ çš„ YOLOv5 best.pt
upn_ckpt_path = "/root/autodl-tmp/resources/resources/upn_large.pth"  # UPN checkpointï¼ŒæŒ‰éœ€ä¿®æ”¹

os.makedirs(output_folder, exist_ok=True)

# ================== å®‰è£… flash-attnï¼ˆå¯é€‰ï¼‰ ==================
if os.path.exists(wheel_path):
    os.system(f"pip install -q {wheel_path}")

# ================== åŠ è½½ VLM-FO1 æ¨¡å‹ ==================
print("ğŸ”¥ åŠ è½½ VLM-FO1 æ¨¡å‹...")
tokenizer, model, image_processors = load_pretrained_model(
    model_path,
    load_8bit=False,
    load_4bit=False,
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# ================== åŠ è½½æœ¬åœ° YOLOv5 æ¨¡å‹ ==================
print("ğŸ”¥ åŠ è½½æœ¬åœ° YOLOv5 æ¨¡å‹ (v7.0)...")
device = "cuda" if torch.cuda.is_available() else "cpu"
yolov5_model = attempt_load(yolov5_weights_path, device=device)
yolov5_model.eval()

# ================== ï¼ˆå¯é€‰ï¼‰åŠ è½½ UPN æ£€æµ‹å™¨ ==================
upn_detector = None
if has_upn and os.path.exists(upn_ckpt_path):
    try:
        print("ğŸ”¥ åŠ è½½ UPN æ£€æµ‹å™¨ï¼Œç”¨äº YOLO æ— æ£€æµ‹æ¡†æ—¶çš„è‡ªåŠ¨æ£€æµ‹...")
        # å®˜æ–¹æ–‡æ¡£è¯´æ˜ï¼šUPNWrapper.__init__(ckpt_path) ä¼šä» checkpoint åˆå§‹åŒ–æ¨¡å‹([DeepWiki](https://deepwiki.com/om-ai-lab/VLM-FO1/2.2-quick-start-guide))
        upn_detector = UPNWrapper(ckpt_path=upn_ckpt_path)
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ UPN æ£€æµ‹å™¨å¤±è´¥ï¼Œå°†ä¸ä¼šä½¿ç”¨ UPNã€‚é”™è¯¯: {e}")
        upn_detector = None
else:
    if has_upn:
        print(f"âš ï¸ æœªæ‰¾åˆ° UPN checkpoint æ–‡ä»¶: {upn_ckpt_path}ï¼Œå°†ä¸ä¼šä½¿ç”¨ UPNã€‚")

# ç±»åˆ«æ ‡ç­¾ï¼ˆå¿…é¡»ä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
fire_label = 0
smoke_label = 1

# ================== å·¥å…·å‡½æ•° ==================
def parse_count_from_text(text: str) -> int:
    """ä» VLM çš„è‡ªç„¶è¯­è¨€å›ç­”ä¸­æå–æ•°é‡ï¼ˆæ”¯æŒè‹±æ–‡æ•°å­—å’Œé˜¿æ‹‰ä¼¯æ•°å­—ï¼‰"""
    text_lower = text.lower().strip()

    # æ˜ç¡®æ— ç›®æ ‡
    if any(word in text_lower for word in
           ["no", "none", "not", "zero", "not visible", "not detected", "no fire", "no smoke"]):
        return 0

    # è‹±æ–‡æ•°å­—æ˜ å°„ï¼ˆè¦†ç›–å¸¸è§å€¼ï¼‰
    word_to_num = {
        "one": 1, "two": 2, "three": 3, "four": 4, "five": 5,
        "six": 6, "seven": 7, "eight": 8, "nine": 9, "ten": 10,
        "multiple": 2, "several": 3, "many": 5, "a lot": 5
    }

    for word, num in word_to_num.items():
        if word in text_lower:
            return num

    # å°è¯•æå–é˜¿æ‹‰ä¼¯æ•°å­—
    numbers = re.findall(r'\d+', text)
    if numbers:
        return int(numbers[0])

    # é»˜è®¤ï¼šå¦‚æœæåˆ°å¯¹è±¡ä½†æ²¡ç»™æ•°é‡ï¼Œè‡³å°‘ç®— 1
    if any(obj in text_lower for obj in ["fire", "smoke", "flame", "burn", "smoke"]):
        return 1

    return 0


def get_yolov5_bboxes(model, img_path, conf_thresh=0.3):
    """ä½¿ç”¨æœ¬åœ° YOLOv5 æ¨ç†ï¼Œè¿”å› [x1, y1, x2, y2] åˆ—è¡¨ å’Œ åŸå§‹å›¾åƒ shape"""
    img0 = cv2.imread(img_path)
    if img0 is None:
        return [], None

    # é¢„å¤„ç†
    img = letterbox(img0, 640, stride=32, auto=True)[0]
    img = img.transpose((2, 0, 1))
    img = np.ascontiguousarray(img)
    img = torch.from_numpy(img).to(device).float() / 255.0
    if img.ndimension() == 3:
        img = img.unsqueeze(0)

    # æ¨ç† + NMS
    with torch.no_grad():
        pred = model(img)[0]
    pred = non_max_suppression(pred, conf_thresh, 0.45, classes=None, agnostic=False)

    bboxes = []
    for det in pred:
        if len(det):
            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()
            for *xyxy, conf, cls in det:
                if float(conf) > conf_thresh:
                    x1, y1, x2, y2 = map(int, xyxy)
                    bboxes.append([x1, y1, x2, y2])
    return bboxes, img0.shape


def detect_and_count(object_name, bbox_list, img_path):
    """
    ä½¿ç”¨ VLM-FO1 åœ¨ç»™å®š bbox_list ä¸Šåšæ£€æµ‹å’Œè®¡æ•°ã€‚
    è¿”å›:
        - bboxes: List[List[float]] æ ¼å¼ä¸º [[x, y, w, h], ...]ï¼ˆVLM è¾“å‡ºä¸­å±äºè¯¥ object_name çš„ bboxï¼‰
        - count: int
    """
    # ---------- æ£€æµ‹ ----------
    detect_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": OD_template.format(object_name)},
            ],
            "bbox_list": bbox_list,
        }
    ]

    # ---------- è®¡æ•° ----------
    count_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": f"How many {object_name} are there in this image?"},
            ],
            "bbox_list": bbox_list,
        }
    ]

    detect_kwargs = prepare_inputs(
        model_path, model, image_processors, tokenizer, detect_messages,
        max_tokens=4096, top_p=0.05, temperature=0.0, do_sample=False
    )
    with torch.inference_mode():
        detect_output_ids = model.generate(**detect_kwargs)
        detect_outputs = tokenizer.decode(
            detect_output_ids[0, detect_kwargs['inputs'].shape[1]:]
        ).strip()

    label_to_bboxes = extract_predictions_to_bboxes(detect_outputs, bbox_list)
    bboxes = label_to_bboxes.get(object_name.lower(), [])

    count_kwargs = prepare_inputs(
        model_path, model, image_processors, tokenizer, count_messages,
        max_tokens=4096, top_p=0.05, temperature=0.0, do_sample=False
    )
    with torch.inference_mode():
        count_output_ids = model.generate(**count_kwargs)
        count_outputs = tokenizer.decode(
            count_output_ids[0, count_kwargs['inputs'].shape[1]:]
        ).strip()

    count = parse_count_from_text(count_outputs)

    return bboxes, count


def vlm_xywh_to_xyxy(bbox_xywh):
    if len(bbox_xywh) < 4:
        return None
    x, y, w, h = bbox_xywh[:4]
    if w <= 0 or h <= 0:
        return None
    return int(x), int(y), int(x + w), int(y + h)


def compute_iou(box1, box2):
    x1_min, y1_min, x1_max, y1_max = box1
    x2_min, y2_min, x2_max, y2_max = box2
    inter_x1 = max(x1_min, x2_min)
    inter_y1 = max(y1_min, y2_min)
    inter_x2 = min(x1_max, x2_max)
    inter_y2 = min(y1_max, y2_max)
    inter_w = max(0, inter_x2 - inter_x1)
    inter_h = max(0, inter_y2 - inter_y1)
    inter_area = inter_w * inter_h
    area1 = (x1_max - x1_min) * (y1_max - y1_min)
    area2 = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = area1 + area2 - inter_area
    return inter_area / union_area if union_area > 0 else 0.0


def fuse_results(yolov5_dets, vlm_fire_bboxes, vlm_smoke_bboxes, fire_count, smoke_count, iou_threshold=0.3):
    """
    YOLO + VLM èåˆï¼š
    - å½“ YOLO æœ‰æ¡†æ—¶ï¼Œç”¨ IoU åˆ¤æ–­æ˜¯å¦ä¸ VLM æ¡†é‡åˆï¼Œé‡åˆåˆ™æå‡ç½®ä¿¡åº¦ï¼›
    - å½“ YOLO æ— æ¡†ä½† VLM æœ‰ç»“æœæ—¶ï¼Œç›´æ¥ä½¿ç”¨ VLM æ¡†ã€‚
    """
    fused_bboxes = []

    def process_vlm_bboxes(vlm_dets):
        processed = []
        for bbox_xywh in vlm_dets:
            xyxy = vlm_xywh_to_xyxy(bbox_xywh)
            if xyxy is None or xyxy[0] >= xyxy[2] or xyxy[1] >= xyxy[3]:
                continue
            processed.append((xyxy[0], xyxy[1], xyxy[2], xyxy[3], 0.9))
        return processed

    vlm_fire_xyxy = process_vlm_bboxes(vlm_fire_bboxes) if fire_count > 0 else []
    vlm_smoke_xyxy = process_vlm_bboxes(vlm_smoke_bboxes) if smoke_count > 0 else []

    # æƒ…å†µ1ï¼šYOLOv5 æœ‰æ£€æµ‹
    for x1, y1, x2, y2, conf, cls in yolov5_dets:
        if conf <= 0.15:
            continue
        cls_id = int(cls)
        yolo_box = (x1, y1, x2, y2)
        label_name = None
        should_promote = False

        if cls_id == fire_label and fire_count > 0:
            label_name = "Fire"
            for xb in vlm_fire_xyxy:
                if compute_iou(yolo_box, xb[:4]) > iou_threshold:
                    should_promote = True
                    break
        elif cls_id == smoke_label and smoke_count > 0:
            label_name = "Smoke"
            for xb in vlm_smoke_xyxy:
                if compute_iou(yolo_box, xb[:4]) > iou_threshold:
                    should_promote = True
                    break
        else:
            continue

        if label_name is None:
            continue

        final_conf = 0.9 if should_promote else conf
        fused_bboxes.append({
            "x1": x1, "y1": y1, "x2": x2, "y2": y2,
            "conf": final_conf, "label": label_name
        })

    # æƒ…å†µ2ï¼šå›é€€åˆ° VLMï¼ˆæ—  YOLO æ¡†æˆ– YOLO æ¡†æœªé€šè¿‡è¿‡æ»¤ï¼‰
    if not fused_bboxes and (fire_count > 0 or smoke_count > 0):
        for x1, y1, x2, y2, conf in vlm_fire_xyxy:
            fused_bboxes.append({"x1": x1, "y1": y1, "x2": x2, "y2": y2, "conf": conf, "label": "Fire"})
        for x1, y1, x2, y2, conf in vlm_smoke_xyxy:
            fused_bboxes.append({"x1": x1, "y1": y1, "x2": x2, "y2": y2, "conf": conf, "label": "Smoke"})

    return fused_bboxes


def get_detailed_description(object_name, img_path):
    """
    ä½¿ç”¨ VLM-FO1 ç”Ÿæˆè¯¦ç»†æè¿°ã€‚
    ä¸ºé¿å… bbox_list ç¼ºå¤±å¯¼è‡´å´©æºƒï¼Œè¿™é‡Œæ€»æ˜¯ç”¨æ•´å›¾ bbox ä½œä¸º fallbackã€‚
    æ— è®ºæ˜¯å¦å­˜åœ¨è¯¥ object_nameï¼ˆfire/smokeï¼‰ï¼Œéƒ½ä¼šå°è¯•æè¿°ã€‚
    """
    img_temp = Image.open(img_path)
    w, h = img_temp.size
    bbox_list = [[0, 0, w, h]]  # [x, y, width, height]

    description_messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": img_path}},
                {"type": "text", "text": f"Describe the {object_name} in this image."},
            ],
            "bbox_list": bbox_list,
        }
    ]
    kwargs = prepare_inputs(
        model_path, model, image_processors, tokenizer, description_messages,
        max_tokens=4096, top_p=0.9, temperature=0.7, do_sample=True
    )
    with torch.inference_mode():
        output_ids = model.generate(**kwargs)
        outputs = tokenizer.decode(
            output_ids[0, kwargs['inputs'].shape[1]:]
        ).strip()
    return outputs


# ================== ä¸»æµç¨‹ ==================
for img_filename in os.listdir(image_folder):
    if not img_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
        continue

    img_path = os.path.join(image_folder, img_filename)
    print(f"\n==============================")
    print(f"å¤„ç†å›¾åƒ: {img_path}")

    # --- Step 1: YOLOv5 æ¨ç† ---
    yolov5_bboxes_coords, orig_shape = get_yolov5_bboxes(yolov5_model, img_path, conf_thresh=0.3)
    if orig_shape is None:
        print(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒï¼Œè·³è¿‡: {img_path}")
        continue

    # --- Step 2: æ„å»º bbox_listï¼ˆä¼˜å…ˆ YOLOï¼Œå…¶æ¬¡ UPNï¼Œæœ€åæ•´å›¾ï¼‰---
    bbox_list = []

    if yolov5_bboxes_coords:
        # æœ‰ YOLO æ£€æµ‹æ¡†ï¼šæ­£å¸¸ä½¿ç”¨ [x1,y1,x2,y2] â†’ [x,y,w,h]
        print(f"âœ… YOLO æ£€æµ‹åˆ° {len(yolov5_bboxes_coords)} ä¸ªæ¡†ï¼Œç”¨äº FO1 æ¨ç†ã€‚")
        bbox_list = [[x1, y1, x2 - x1, y2 - y1] for x1, y1, x2, y2 in yolov5_bboxes_coords]

    elif upn_detector is not None:
        # æ²¡æœ‰ YOLO æ¡†ï¼Œå°è¯•ä½¿ç”¨ UPN è‡ªåŠ¨æ£€æµ‹ï¼ˆFO1 å®˜æ–¹æ¨èçš„ Path2ï¼‰([DeepWiki](https://deepwiki.com/om-ai-lab/VLM-FO1/2.2-quick-start-guide))
        try:
            print("ğŸ” YOLO æ— æ£€æµ‹æ¡†ï¼Œä½¿ç”¨ UPN æ£€æµ‹å™¨ç”Ÿæˆ proposals ...")
            pil_img = Image.open(img_path).convert("RGB")
            # prompt_type ä½¿ç”¨ fine_grained_promptï¼Œé€‚åˆç²¾ç»†ç›®æ ‡æ£€æµ‹å’Œè®¡æ•°([DeepWiki](https://deepwiki.com/om-ai-lab/VLM-FO1/2.2-quick-start-guide))
            upn_raw = upn_detector.inference(pil_img, prompt_type="fine_grained_prompt")
            upn_filtered = upn_detector.filter(upn_raw, min_score=0.4, nms_value=0.8)

            # å®˜æ–¹æ–‡æ¡£è¯´æ˜ï¼šfilter è¾“å‡ºåŒ…å« boxesï¼ˆ[x1,y1,x2,y2]ï¼‰å’Œ scores ç­‰å­—æ®µ([DeepWiki](https://deepwiki.com/om-ai-lab/VLM-FO1/2.2-quick-start-guide))
            upn_boxes_xyxy = upn_filtered.get("boxes", []) if isinstance(upn_filtered, dict) else []
            print(f"âœ… UPN æ£€æµ‹åˆ° {len(upn_boxes_xyxy)} ä¸ª proposalsã€‚")

            if len(upn_boxes_xyxy) > 0:
                bbox_list = [[float(x1), float(y1), float(x2 - x1), float(y2 - y1)]
                             for x1, y1, x2, y2 in upn_boxes_xyxy]

        except Exception as e:
            print(f"âš ï¸ UPN æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œå°†é€€å›æ•´å›¾ bboxã€‚é”™è¯¯: {e}")
            bbox_list = []

    if not bbox_list:
        # YOLO ä¸ UPN éƒ½æ²¡æœ‰ç»™å‡ºæœ‰æ•ˆ bboxï¼Œç”¨æ•´å›¾ fallback
        h, w = orig_shape[:2]
        print("âš ï¸ YOLO å’Œ UPN å‡æ— æœ‰æ•ˆæ£€æµ‹æ¡†ï¼Œä½¿ç”¨æ•´å›¾ä½œä¸ºå•ä¸€ bboxã€‚")
        bbox_list = [[0, 0, w, h]]

    # --- Step 3: è°ƒç”¨ VLM-FO1 è¿›è¡Œ fire / smoke æ£€æµ‹ä¸è®¡æ•° ---
    try:
        vlm_fire_bboxes, fire_count = detect_and_count("fire", bbox_list, img_path)
        vlm_smoke_bboxes, smoke_count = detect_and_count("smoke", bbox_list, img_path)
    except Exception as e:
        print(f"âŒ VLM æ¨ç†å‡ºé”™ï¼ˆè·³è¿‡æ­¤å›¾ï¼‰: {e}")
        continue

    # --- Step 4: æ„å»º yolov5_detsï¼ˆä»…å½“ YOLO æœ‰æ¡†æ—¶ï¼‰---
    yolov5_dets = []
    for (x1, y1, x2, y2) in yolov5_bboxes_coords:
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ›¿æ¢æˆçœŸå®çš„ conf å’Œ cls
        yolov5_dets.append([x1, y1, x2, y2, 0.5, fire_label])  # ç®€åŒ–ï¼šå…¨éƒ¨å½“ä½œ fire

    # --- Step 5: èåˆ YOLO + VLM ç»“æœ ---
    fused_bboxes = fuse_results(
        yolov5_dets,
        vlm_fire_bboxes,
        vlm_smoke_bboxes,
        fire_count,
        smoke_count,
        iou_threshold=0.3
    )

    # --- Step 6: ç»˜å›¾å¹¶ä¿å­˜ ---
    image = Image.open(img_path).convert("RGB")
    img_np = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    for box in fused_bboxes:
        x1 = int(box["x1"])
        y1 = int(box["y1"])
        x2 = int(box["x2"])
        y2 = int(box["y2"])
        conf = float(box["conf"])
        label = box["label"]

        color = (0, 255, 0) if label == "Fire" else (0, 0, 255)
        cv2.rectangle(img_np, (x1, y1), (x2, y2), color, 2)
        cv2.putText(
            img_np,
            f"{label} {conf:.2f}",
            (x1, max(y1 - 10, 0)),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            color,
            2
        )

    output_filename = os.path.splitext(img_filename)[0] + "_result.jpg"
    output_path = os.path.join(output_folder, output_filename)
    cv2.imwrite(output_path, img_np)

    # --- Step 7: ç”Ÿæˆè¯¦ç»†æè¿°ï¼ˆâ—æ— è®ºæ˜¯å¦æœ‰æ£€æµ‹æ¡† / æ˜¯å¦æ£€æµ‹åˆ° fire/smoke éƒ½ä¼šæ‰§è¡Œï¼‰---
    try:
        detailed_description_fire = get_detailed_description("fire", img_path)
    except Exception as e:
        detailed_description_fire = f"[Fire æè¿°å¤±è´¥: {e}]"

    try:
        detailed_description_smoke = get_detailed_description("smoke", img_path)
    except Exception as e:
        detailed_description_smoke = f"[Smoke æè¿°å¤±è´¥: {e}]"

    print(f"Fire Description: {detailed_description_fire}")
    print(f"Smoke Description: {detailed_description_smoke}")

    description_filename = os.path.splitext(img_filename)[0] + "_description.txt"
    description_path = os.path.join(output_folder, description_filename)
    with open(description_path, "w", encoding="utf-8") as f:
        f.write(f"Fire Description: {detailed_description_fire}\n")
        f.write(f"Smoke Description: {detailed_description_smoke}\n")

    print(f"ç«çš„æ•°é‡: {fire_count}")
    print(f"çƒŸçš„æ•°é‡: {smoke_count}")
    print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"æè¿°æ–‡ä»¶å·²ä¿å­˜åˆ°: {description_path}")

    if len(fused_bboxes) == 0 and fire_count == 0 and smoke_count == 0:
        print("æœªæ£€æµ‹åˆ°ä»»ä½• Fire æˆ– Smokeã€‚")